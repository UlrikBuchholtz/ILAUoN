<?xml version="1.0" encoding="UTF-8"?>

<!--********************************************************************
Copyright 2023 Ulrik Buchholtz

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3 or
any later version published by the Free Software Foundation.  A copy of
the license is included in gfdl.xml.
*********************************************************************-->

<section xml:id="matrix-coords">
  <title>Invertible matrices and Coordinate Systems</title>

  <objectives>
    <ol>
      <li>Learn to use invertible matrices to convert between coordinate systems.</li>
      <li>Learn to represent linear transformations with respect to given bases.</li>
      <li><em>Recipes:</em> compute the <m>(\cB,\cC)</m>-matrix of a linear transformation.</li>
      <li><em>Vocabulary:</em> <term><m>(\cB,\cC)</m>-matrix</term>.</li>
    </ol>
  </objectives>

  <p>
    In this section, we study matrix representations for linear transformations with respect to bases for the domain and the codomain. We'll also get another perspective on bases in terms of the linear transformations they represent, and we'll see how to convert between different coordinate systems using matrices and their inverses.
  </p>

  <fact hide-type="true" xml:id="linear-trans-pick-columns">
    <title>Bases and one-to-one linear transformations</title>
    <p>
      If <m>B</m> is an <m>m\times n</m> matrix with columns <m>v_1,v_2,\ldots,v_n</m>,
      then <m>\cB = (v_1,\ldots,v_n)</m> is a basis for the column span <m>V = \Col(B)</m>
      if and only if the linear transformation <m>T : \R^n \to \R^m</m>,
      <m>T(x)=Bx</m> is one-to-one.
    </p>
  </fact>

  <p>
    Indeed, the columns of <m>B</m> span <m>V</m> by definition, and they're
    linearly independent if and only if <m>T</m> is one-to-one, using
    <xref ref="matrix-trans-one-to-one"/>.
  </p>

  <p>
    In particular, a list <m>\cB = (v_1,\ldots,v_n)</m> of vectors in <m>\R^n</m>
    forms a basis for all of <m>\R^n</m> if and only if the square <m>n\times n</m>
    matrix <m>B</m> with columns <m>v_1,\ldots,v_n</m> is invertible.
    These columns are clearly the coordinate vectors of the <m>v_i</m>
    with respect to the standard basis <m>\cE = (e_1,\ldots,e_n)</m> for <m>\R^n</m>.
    Conversely, the columns of the inverse matrix <m>B^{-1}</m>
    are the <m>\cB</m>-coordinates of the standard basis vectors:
    <me>
      B^{-1} = \mat[c]{| | ,, |; {}_\cB[e_1] {}_\cB[e_2] \cdots, {}_\cB[e_n]; | | ,, |}.
    </me>
    This is because <m>BB^{-1} = I_n</m>, so <m>B({}_\cB[e_i]) = e_i</m>,
    which is the definition of <m>{}_\cB[e_i]</m> being the <m>\cB</m>-coordinates
    of <m>e_i</m>.
    Multiplying by <m>B^{-1}</m> and taking linear combinations we get:
  </p>

  <bluebox>
    <p>
      If <m>B</m> is an invertible <m>n\times n</m> square matrix,
      whose columns thus form a basis <m>\cB</m> of <m>\R^n</m>,
      the <m>\cB</m>-coordinates of any vector <m>x\in\R^n</m> is given by:
      <me>{}_\cB[x] = B^{-1}x</me>
    </p>
  </bluebox>

  <definition xml:id="defn-BCmatrix">
    <idx><h><m>(\cB,\cC)</m>-matrix</h><h>definition of</h></idx>
    <notation><usage>{}_\cC[T]_\cB</usage><description>The <m>(\cB,\cC)</m>-matrix of a linear transformation</description></notation>
    <statement>
      <p>
        Suppose we're given bases <m>\cB = (v_1,\ldots,v_n)</m> for <m>\R^n</m>
        and <m>\cC = (w_1,\ldots,w_m)</m> for <m>\R^m</m>.
        Let <m>T : \R^n \to \R^m</m> be a linear transformation.
        The <term><m>(\cB,\cC)</m>-matrix of <m>T</m></term> is the <m>m\times n</m>
        matrix
        <me>
          {}_\cC[T]_\cB =
          \mat[c]{| | ,, |;
          {}_\cC[T(v_1)] {}_\cC[T(v_2)] \cdots, {}_\cC[T(v_n)];
          | | ,, | }.
        </me>
      </p>
    </statement>
  </definition>

  <p>
    This generalizes the definition of the standard matrix of <m>T</m>:
    If we let <m>\cB=\cE_n</m> and <m>\cC=\cE_m</m> be the standard bases
    for <m>\R^n</m> and <m>\R^m</m>, then <m>{}_{\cE_m}[T]_{\cE_n} = [T]</m>,
    the standard matrix of <m>T</m>.
  </p>

  <p>
    By the discussion above, if <m>\cE</m> is the standard basis for <m>\R^n</m>
    and <m>\cB = (v_1,\ldots,v_n)</m> is another basis for <m>\R^n</m>,
    so the square <m>n\times n</m> matrix <m>B</m> with columns
    <m>v_1,\ldots,v_n</m> is invertible,
    then <m>B = {}_\cE[\Id_{\R^n}]_\cB</m>
    and <m>B^{-1} = {}_\cB[\Id_{\R^n}]_\cE</m>.
  </p>

  <bluebox>
    <p>
      If <m>B</m> is an invertible <m>n\times n</m> square matrix,
      whose columns thus form a basis <m>\cB</m> of <m>\R^n</m>,
      and <m>C</m> is an invertible <m>m\times m</m> square matrix,
      whose columns thus form a basis <m>\cC</m> of <m>\R^m</m>,
      then the <m>(\cB,\cC)</m>-matrix of a linear transformation
      <m>T : \R^n \to \R^m</m> is given by the matrix product
      <me>{}_\cC[T]_\cB = C^{-1}[T]B,</me>
      where <m>[T]</m> is the standard matrix for <m>T</m>.
    </p>
  </bluebox>

  <fact hide-type="true">
    <title>Facts about <m>(\cB,\cC)</m>-matrices</title>
    <statement>
      <p>
        Let <m>T : \R^n\to\R^m</m> and <m>U : \R^p\to\R^n</m> be linear transformations,
        and let <m>\cB,\cC,\cD</m> be bases for <m>\R^p,\R^n,\R^m</m>, respectively.
        Then:
        <ol>
          <li>
            <m>{}_\cD[T(v)] = ({}_\cD[T]_\cC)({}_\cC[v])</m> for any <m>v\in\R^n</m>.
          </li>
          <li>
            <m>{}_\cD[T \circ U]_\cB = ({}_\cD[T]_\cC)({}_\cC[U]_\cB)</m>
          </li>
          <li>
            If <m>T</m> is invertible (so <m>n=m</m>),
            then <m>{}_\cC[T^{-1}]_\cD = ({}_\cD[T]_\cC)^{-1}</m>.
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <p>
        <ol>
          <li>
            <m>{}_\cD[T(v)] = D^{-1}(T(v)) = D^{-1}[T]v
            = D^{-1}[T]CC^{-1}v = ({}_\cD[T]_\cC)({}_\cC[v])</m>
          </li>
          <li>
            <m>{}_\cD[T \circ U]_\cB = D^{-1}[T][U]B
            = D^{-1}[T]CC^{-1}[U] = ({}_\cD[T]_\cC)({}_\cC[U]_\cB)</m>
          </li>
          <li>
            This follows directly from 2:
            <m>({}_\cC[T^{-1}]_\cD)({}_\cD[T]_\cC)^{-1} = {}_\cC[T^{-1} \circ T]_\cC
            = {}_\cC[\Id_{\R^n}]_\cC = I_n</m>.
          </li>
        </ol>
      </p>
    </proof>
  </fact>

  <theorem xml:id="rank-normal-form">
    <statement>
      <p>
        Suppose <m>T : \R^n \to \R^m</m> is a linear map.
        Then we can pick bases, <m>\cB</m> for <m>\R^n</m>
        and <m>\cC</m> for <m>\R^m</m>, relative to which the
        <m>(\cB,\cC)</m>-matrix for <m>T</m> is diagonal with <m>\rank(T)</m>
        ones and zeroes everywhere else:
        <me>{}_\cC[T]_\cB = \mat[c]{1 0 \cdots, 0; 0 1 \cdots, 0;
              \vdots, \vdots, \ddots, \vdots; 0 0 \cdots, 0}.</me>
      </p>
    </statement>
    <proof>
      <p>
        This is essentially just another application of the
        <xref ref="linindep-pivot-cols"/>.
        If <m>A</m> is the standard matrix for <m>T</m>, say,
        <!-- https://ulrikbuchholtz.dk/ila/demos/rrinter.html?mat=1,2,0,3:0,0,1,-1:0,0,0,0&ops=r1:2:2,r1:-1:0,r0:2:1,r0:-1:2&cur=4 -->
        <me>
          A = \mat{1 2 -1 4; 2 4 -1 7; -1 -2 3 -6}
          \quad\xrightarrow{\text{RREF}}\quad\mat{1 2 0 3; 0 0 1 -1; 0 0 0 0},
        </me>
        then we take the pivot columns of <m>A</m> (here, the first and third columns),
        as these form a basis for the image, and extend to a basis for all of <m>\R^m</m>,
        again using the pivotal theorem. In the example, we get:
        <me>
          \cC = \left(\vec{1 2 -1},\vec{-1 -1 3},\vec{1 0 0}\right)
        </me>
        To get <m>\cB</m>, we pick the standard basis vectors corresponding to the
        pivot columns of <m>A</m> and extend that a basis for all <m>\R^n</m>
        by appending a basis for the kernel of <m>T</m>,
        which we find using the RREF of its standard matrix. Here we get:
        <me>
          \cB = \left(\vec{1 0 0 0},\vec{0 0 1 0}, \vec{-3 0 1 1},
          \vec{-2 1 0 0}\right)
        </me>
        It's easy to see that this works, and we can check this by forming the matrices
        corresponding to <m>\cB</m> and <m>\cC</m> and computing:
        <me>
          \begin{split}
          {}_\cC[T]_\cB \amp= C^{-1}AB = \mat{1 -1 1;2 -1 0;-1 3 0}^{-1}
          \mat{1 2 -1 4; 2 4 -1 7; -1 -2 3 -6}
          \mat{1 0 -3 -2; 0 0 0 1; 0 1 1 0; 0 0 1 0} \\
          \amp= \mat{1 0 0 0; 0 1 0 0; 0 0 0 0}.
          \end{split}
        </me>
      </p>
    </proof>
  </theorem>

  <p>
    This is a very simple kind of diagonalisation theorem,
    which is made possible by the fact that we can choose bases
    for the domain and the codomain independently.
    In <xref ref="chap-eigenvalues"/> we'll see that the situation is much more subtle
    for linear transformations <m>T : \R^n \to \R^n</m> where the domain and
    the codomain coincide and we want to use the <em>one</em> basis for both.
  </p>

  <!--
      <p>TODO: general vector spaces? examples! passive versus active transformations.</p>
  -->
</section>
